# Preliminaries{#sec:prelims}

## Scope and contribution of the study{#sec:prelims:scope}

\noindent We contribute to the topic by formalizing a geometrical framework for unsupervised outlier detection, which generalizes a concept for outlier detection in functional data analysis (FDA) [@herrmann2021geometric]. The framework builds on principles from *manifold learning* [@ma2011manifold; @lee2007nonlinear], i.e., dimension reduction methods which infer the intrinsic lower-dimensional manifold structure of high-dimensional data and yield low dimensional vector representations of the data. This perspective allows us to formalize structural and distributional outliers jointly in a single mathematical framework, where structural outliers are data that are separate from the main data manifold, and distributional outliers are data that are situated at the periphery of, but still on the main data manifold. 
Based on this framework, we can shed more light on two specific aspects of outlier detection.

First, there seems to be a lack of clarity about what defines outliers, evidenced also by the plethora of terms used to describe the issue [@zimek2018there]. Several recent reviews on the topic also point out this conceptual ambiguity [@goldstein2016comparative; @zimek2018there; @unwin2019multivariate]. In particular, the comprehensive overview of Zimek and Filzmoser [-@zimek2018there, p. 3] devotes a complete section to the question "What an 'outlier' possibly means".  They define "true outliers" as objects "that have been 'generated by a different mechanism' than the remainder or major part of the data or than the whatsoever defined reference set" and distinguish them from "objects that appear to be outliers (independent of whether or not they actually are (true) outliers") [@zimek2018there, p. 6]. As we will show, the proposed geometrical framework provides suitable mathematical terminology to delineate "true" and "apparent" outliers much more cleanly and thus reduces some of the conceptual confusion that surrounds the topic.  
Second, our framework also suggests that outlier detection in high dimensional (and/or non-tabular) data is not necessarily more challenging than in low dimensional settings once the underlying manifold structure is recovered and exploited. This is important because high dimensionality is often reported to be particularly problematic for outlier detection and  many outlier detection methods break down or at least face particular challenges in such settings [@aggarwal2001outlier; @zimek2012survey, e.g.; @ro2015outlier; @goldstein2016comparative; @aggarwal2017outlier; @xu2018comparison; @kamalov2020outlier; @thudumu2020comprehensive; @navarro2021high].  
To highlight these aspects, we focus on functional data as an example of high dimensional data. Functional data analysis [@ramsay2005functional, e.g.] deals with data that are (discretized) realizations of stochastic processes over a compact domain. In practice, functional data arises in a variety of domains and applications, such as electrocardiograms (ECG) in the medical domain [@goldberger2000physiobank] or spectrographical measurements in chemometrics [@large2018] and astrophysics [@rebbapragada2009finding]. For our exposition, we focus mostly on functional data since it is comparatively easy to visualize in bulk, but we  include examples of more general data types such as graph and image data.
Since our framework rests on the manifold assumption, it is specifically useful for such structured high-dimensional data but its geometrical foundation ensures that it can be applied to any data type for which suitable dissimilarity or distance measures can be defined. 
Our framework is fully general and does not rely on a specific combination of manifold learning and outlier detection methods. To demonstrate its practical performance, we show that one of the simplest and most established manifold learning methods -- Multidimensional Scaling (MDS) [@cox2008multidimensional] -- combined with a standard outlier detection algorithm -- Local Outlier Factors (LOF) [@breunig2000lof] -- already yields a flexible, reliable and  generally applicable recipe for outlier detection and visualization in complex, high-dimensional data. 

## Background and related work{#sec:prelims:background}

The fundamental assumption of manifold learning is that the high-dimensional data observed in a $\obsdim$-dimensional space $\hdspace$ actually lie on a $d$-dimensional manifold $\mani \subset \hdspace$ with $d < \obsdim$. Manifold learning methods yield an *embedding* function $e:\hdspace \to \embedspace$ from the high-dimensional data space to a low-dimensional embedding space $\embedspace$ such that the configuration of embedded data reflects the characteristics of $\M$. The terms manifold learning and nonlinear dimension reduction are often used interchangeably [@ma2011manifold; @lee2007nonlinear]. 
Typically, the fundamental step is to compute distances between the high-dimensional observations. Methods based on this approach are, for example, Multidimensional Scaling (MDS) [@cox2008multidimensional], Isomap [@tenenbaum2000global], diffusion maps [@coifman2006diffusion], local linear embeddings [@roweis2000nonlinear], Laplacian eigenmaps [@belkin2003laplacian], t-distributed stochastic neighborhood embeddings (t-SNE) [@maaten2008visualizing], and uniform manifold approximation and projection (UMAP) [@mcinnes2018umap], to name only a few. The methods differ in how they infer the manifold structure from these distances and how they obtain low-dimensional embedding vectors from these.  
Despite their promising results in other settings, manifold learning methods have not found application for outlier detection to a significant extent so far. @kandanaarachchi2020dimension defined an outlier detection method explicitly based on dimension reduction, while @pang2018learning make use of ranking model-based representation learning. However, they do not provide a general conceptual framework and focus on tabular data. For functional data, @xie2017geometric introduced a geometric approach that decomposes  functional observations into amplitude, phase and shift components in order to identify specific types of outliers. However, the approach is only applicable to functional data and does not make use of the intrinsic structure of the functional observations from a manifold learning perspective. @ali2019timecluster analyze time series data using 2$\vizdim$-embeddings obtained from manifold methods for outlier detection and clustering and @toivola2010novelty compare specific dimensionality reduction techniques for outlier detection in structural health monitoring, but both focus on practical considerations and do not provide a theoretical underpinning. Another line of work focuses on projection-based outlier detection, for example for high-dimensional Gaussian data [@navarro2021high], financial time series [@loperfido2020kurtosis], or functional data [@ren2017projection]. 
